{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from microscopy.trainers import train_configuration\n",
    "from hydra import compose, initialize\n",
    "import os\n",
    "\n",
    "def load_path(dataset_root, dataset_name, folder):\n",
    "    if folder is not None:\n",
    "        return os.path.join(dataset_root, dataset_name, folder)\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run to specify GPU\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_name =  \"config\" #@param [\"EM_crap_2x_03\", \"EM_crap_2x_05\", \"EM_crap_2x_09\", \"EM_crap_4x_03\", \"EM_crap_4x_05\", \"EM_crap_4x_09\", \"EM_down_2x\", \"EM_down_4x\", \"EM_old_crap_4x\", \"wo_level_1\"] \n",
    "\n",
    "dataset_name = \"F-actin_wo_level1\" #@param [\"LiveFActinDataset\", \"EM\", \"F-actin\", \"ER\", \"MT\", \"MT-SMLM_registered\"]\n",
    "\n",
    "model_name = \"unet\" #@param [\"unet\", \"rcan\", \"dfcan\", \"wdsr\", \"wgan\", \"esrganplus\", \"cddpm\"] \n",
    "batch_size = 4 #@param {type:\"integer\"}\n",
    "num_epochs =  100 #@param {type:\"integer\"}\n",
    "lr = 0.001 #@param {type:\"number\"}\n",
    "discriminator_lr = 0.001 #@param {type:\"number\"}\n",
    "\n",
    "scheduler = 'OneCycle' #@param {'Fixed', 'ReduceOnPlateau', 'OneCycle', 'CosineDecay', 'MultiStepScheduler'}\n",
    "discriminator_scheduler = 'OneCycle' #@param {'Fixed', 'ReduceOnPlateau', 'OneCycle', 'CosineDecay', 'MultiStepScheduler'}\n",
    "optimizer = 'adam'  #@param {'adam', 'adamW', 'adamax', 'rms_prop', 'sgd'}\n",
    "discriminator_optimizer = 'adam'  #@param {'adam', 'adamW', 'adamax', 'rms_prop', 'sgd'}\n",
    "    \n",
    "saving_folder = \"\" #@param {type:\"string\"}\n",
    "vebose = True #@param {type:\"boolean\"}\n",
    "on_memory = True #@param {type:\"boolean\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialize(version_base=None, config_path=\"conf\", job_name=\"test_app\")\n",
    "cfg = compose(config_name=config_name)\n",
    "        \n",
    "cfg.dataset_name = dataset_name\n",
    "\n",
    "train_lr, train_hr, val_lr, val_hr, test_lr, test_hr = cfg.used_dataset.data_paths\n",
    "\n",
    "dataset_root = \"datasets\" if os.path.exists(\"datasets\") else \"../datasets\"\n",
    "train_lr_path = load_path(dataset_root, dataset_name, train_lr)\n",
    "train_hr_path = load_path(dataset_root, dataset_name, train_hr)\n",
    "val_lr_path = load_path(dataset_root, dataset_name, val_lr)\n",
    "val_hr_path = load_path(dataset_root, dataset_name, val_hr)\n",
    "test_lr_path = load_path(dataset_root, dataset_name, test_lr)\n",
    "test_hr_path = load_path(dataset_root, dataset_name, test_hr)\n",
    "\n",
    "cfg.model_name = model_name\n",
    "cfg.hyperparam.batch_size = batch_size\n",
    "cfg.hyperparam.num_epochs = num_epochs\n",
    "cfg.hyperparam.lr = lr\n",
    "cfg.hyperparam.discriminator_lr = discriminator_lr\n",
    "\n",
    "cfg.hyperparam.scheduler = scheduler\n",
    "cfg.hyperparam.discriminator_lr_scheduler = discriminator_scheduler\n",
    "cfg.hyperparam.optimizer = optimizer\n",
    "cfg.hyperparam.discriminator_optimizer = discriminator_optimizer\n",
    "\n",
    "cfg.model.optim.early_stop.patience = num_epochs\n",
    "\n",
    "save_folder = \"scale\" + str(cfg.used_dataset.scale)\n",
    "if cfg.hyperparam.additional_folder is not None:\n",
    "    save_folder += \"_\" + cfg.hyperparam.additional_folder\n",
    "\n",
    "saving_path = os.path.join(\n",
    "    saving_folder, \n",
    "    cfg.dataset_name,\n",
    "    cfg.model_name,\n",
    "    save_folder,\n",
    "    f\"epc{cfg.hyperparam.num_epochs}_btch{cfg.hyperparam.batch_size}_lr{cfg.hyperparam.lr}_optim-{cfg.hyperparam.optimizer}_lrsched-{cfg.hyperparam.scheduler}_seed{cfg.hyperparam.seed}_1\"\n",
    ")\n",
    "\n",
    "test_metric_path = os.path.join(saving_path, \"test_metrics\")\n",
    "if (os.path.exists(test_metric_path) and len(os.listdir(test_metric_path)) > 0):\n",
    "    # In case you want to skip existing configurations\n",
    "    # continue\n",
    "    # In case you want to repeat existing configurations\n",
    "    saving_path = saving_path[:-1] + str(int(saving_path[-1]) + 1) \n",
    "\n",
    "model = train_configuration(\n",
    "    config=cfg,\n",
    "    train_lr_path=train_lr_path,\n",
    "    train_hr_path=train_hr_path,\n",
    "    val_lr_path=val_lr_path,\n",
    "    val_hr_path=val_hr_path,\n",
    "    test_lr_path=test_lr_path,\n",
    "    test_hr_path=test_hr_path,\n",
    "    saving_path=saving_path,\n",
    "    verbose=0, # 0, 1 or 2\n",
    "    data_on_memory=0\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
