{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ihidalgo/miniconda3/envs/microscopy/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/ihidalgo/miniconda3/envs/microscopy/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /home/ihidalgo/miniconda3/envs/microscopy/lib/python3.9/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ihidalgo/miniconda3/envs/microscopy/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /home/ihidalgo/miniconda3/envs/microscopy/lib/python3.9/site-packages/lpips/weights/v0.1/vgg.pth\n",
      "Selecting OpenCL device: NVIDIA GeForce RTX 2080 Ti\n",
      "2.8.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ihidalgo/miniconda3/envs/microscopy/lib/python3.9/site-packages/nanopyx/liquid/__njit__.py:15: UserWarning: Numba is not installed. Using pure python for _njit_interpolate\n",
      "  warnings.warn(\n",
      "/home/ihidalgo/miniconda3/envs/microscopy/lib/python3.9/site-packages/nanopyx/liquid/__njit__.py:15: UserWarning: Numba is not installed. Using pure python for njit_shift_magnify\n",
      "  warnings.warn(\n",
      "/home/ihidalgo/miniconda3/envs/microscopy/lib/python3.9/site-packages/nanopyx/liquid/__njit__.py:15: UserWarning: Numba is not installed. Using pure python for njit_shift_scale_rotate\n",
      "  warnings.warn(\n",
      "/home/ihidalgo/miniconda3/envs/microscopy/lib/python3.9/site-packages/nanopyx/liquid/_le_mandelbrot_benchmark_.py:17: UserWarning: Numba is not installed. Using pure python for _njit_mandelbrot\n",
      "  warnings.warn(\n",
      "/home/ihidalgo/miniconda3/envs/microscopy/lib/python3.9/site-packages/nanopyx/liquid/_le_mandelbrot_benchmark_.py:17: UserWarning: Numba is not installed. Using pure python for njit_mandelbrot\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import csv\n",
    "import os\n",
    "\n",
    "from skimage import io\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint as tf_ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "from pytorch_lightning.trainer import Trainer\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import sys\n",
    "sys.path.append('/data/ihidalgo/MicroscoPy')        \n",
    "\n",
    "from microscopy import datasets\n",
    "from microscopy import utils\n",
    "from microscopy import metrics\n",
    "from microscopy import model_utils\n",
    "from microscopy import optimizer_scheduler_utils\n",
    "from microscopy import tensorflow_callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hydra\n",
    "import os\n",
    "\n",
    "from hydra import compose, initialize\n",
    "from omegaconf import OmegaConf, DictConfig\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "from omegaconf import DictConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'defaults': [{'dataset': 'default.yaml'}, {'model': 'default.yaml'}, {'hyperparam': 'default.yaml'}, '_self_'], 'model.optim.early_stop.patience': '${hyperparam.num_epochs}', 'dataset_name': 'EM', 'model_name': 'unet', 'used_dataset': '${dataset.${dataset_name}}', 'used_model': '${model.${model_name}}', 'used_optim': '${model.optim.${hyperparam.optimizer}}', 'used_optim_d': '${model.optim.${hyperparam.discriminator_optimizer}}', 'used_sched': '${model.optim.${hyperparam.scheduler}}', 'used_sched_d': '${model.optim.${hyperparam.discriminator_lr_scheduler}}'}\n"
     ]
    },
    {
     "ename": "InterpolationKeyError",
     "evalue": "Interpolation key 'dataset.F-actin' not found\n    full_key: used_dataset\n    object_type=dict",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInterpolationKeyError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 62\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[39mreturn\u001b[39;00m cfg, train_lr_path, train_hr_path, val_lr_path, val_hr_path, test_lr_path, test_hr_path, saving_path, \u001b[39m1\u001b[39m\n\u001b[1;32m     61\u001b[0m cfg \u001b[39m=\u001b[39m cfg \u001b[39m=\u001b[39m OmegaConf\u001b[39m.\u001b[39mload(\u001b[39m'\u001b[39m\u001b[39m../conf/config.yaml\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 62\u001b[0m cfg, train_lr_path, train_hr_path, val_lr_path, val_hr_path, test_lr_path, test_hr_path, saving_path, verbose \u001b[39m=\u001b[39m my_app(cfg)\n",
      "Cell \u001b[0;32mIn[27], line 12\u001b[0m, in \u001b[0;36mmy_app\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mfor\u001b[39;00m dataset_name \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mF-actin\u001b[39m\u001b[39m\"\u001b[39m]:  \n\u001b[1;32m     11\u001b[0m     cfg\u001b[39m.\u001b[39mdataset_name \u001b[39m=\u001b[39m dataset_name\n\u001b[0;32m---> 12\u001b[0m     train_lr, train_hr, val_lr, val_hr, test_lr, test_hr \u001b[39m=\u001b[39m cfg\u001b[39m.\u001b[39;49mused_dataset\u001b[39m.\u001b[39mdata_paths\n\u001b[1;32m     14\u001b[0m     dataset_root \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdatasets\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(\u001b[39m\"\u001b[39m\u001b[39mdatasets\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m../datasets\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     15\u001b[0m     train_lr_path \u001b[39m=\u001b[39m load_path(dataset_root, dataset_name, train_lr)\n",
      "File \u001b[0;32m~/miniconda3/envs/microscopy/lib/python3.9/site-packages/omegaconf/dictconfig.py:359\u001b[0m, in \u001b[0;36mDictConfig.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    355\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_and_raise(\n\u001b[1;32m    356\u001b[0m         key\u001b[39m=\u001b[39mkey, value\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, cause\u001b[39m=\u001b[39me, type_override\u001b[39m=\u001b[39mConfigAttributeError\n\u001b[1;32m    357\u001b[0m     )\n\u001b[1;32m    358\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 359\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_format_and_raise(key\u001b[39m=\u001b[39;49mkey, value\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, cause\u001b[39m=\u001b[39;49me)\n",
      "File \u001b[0;32m~/miniconda3/envs/microscopy/lib/python3.9/site-packages/omegaconf/base.py:231\u001b[0m, in \u001b[0;36mNode._format_and_raise\u001b[0;34m(self, key, value, cause, msg, type_override)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_format_and_raise\u001b[39m(\n\u001b[1;32m    224\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    225\u001b[0m     key: Any,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    229\u001b[0m     type_override: Any \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    230\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 231\u001b[0m     format_and_raise(\n\u001b[1;32m    232\u001b[0m         node\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m    233\u001b[0m         key\u001b[39m=\u001b[39;49mkey,\n\u001b[1;32m    234\u001b[0m         value\u001b[39m=\u001b[39;49mvalue,\n\u001b[1;32m    235\u001b[0m         msg\u001b[39m=\u001b[39;49m\u001b[39mstr\u001b[39;49m(cause) \u001b[39mif\u001b[39;49;00m msg \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m msg,\n\u001b[1;32m    236\u001b[0m         cause\u001b[39m=\u001b[39;49mcause,\n\u001b[1;32m    237\u001b[0m         type_override\u001b[39m=\u001b[39;49mtype_override,\n\u001b[1;32m    238\u001b[0m     )\n\u001b[1;32m    239\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/microscopy/lib/python3.9/site-packages/omegaconf/_utils.py:899\u001b[0m, in \u001b[0;36mformat_and_raise\u001b[0;34m(node, key, value, msg, cause, type_override)\u001b[0m\n\u001b[1;32m    896\u001b[0m     ex\u001b[39m.\u001b[39mref_type \u001b[39m=\u001b[39m ref_type\n\u001b[1;32m    897\u001b[0m     ex\u001b[39m.\u001b[39mref_type_str \u001b[39m=\u001b[39m ref_type_str\n\u001b[0;32m--> 899\u001b[0m _raise(ex, cause)\n",
      "File \u001b[0;32m~/miniconda3/envs/microscopy/lib/python3.9/site-packages/omegaconf/_utils.py:797\u001b[0m, in \u001b[0;36m_raise\u001b[0;34m(ex, cause)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    796\u001b[0m     ex\u001b[39m.\u001b[39m__cause__ \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 797\u001b[0m \u001b[39mraise\u001b[39;00m ex\u001b[39m.\u001b[39mwith_traceback(sys\u001b[39m.\u001b[39mexc_info()[\u001b[39m2\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/envs/microscopy/lib/python3.9/site-packages/omegaconf/dictconfig.py:351\u001b[0m, in \u001b[0;36mDictConfig.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m()\n\u001b[1;32m    350\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 351\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_impl(\n\u001b[1;32m    352\u001b[0m         key\u001b[39m=\u001b[39;49mkey, default_value\u001b[39m=\u001b[39;49m_DEFAULT_MARKER_, validate_key\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m\n\u001b[1;32m    353\u001b[0m     )\n\u001b[1;32m    354\u001b[0m \u001b[39mexcept\u001b[39;00m ConfigKeyError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    355\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_and_raise(\n\u001b[1;32m    356\u001b[0m         key\u001b[39m=\u001b[39mkey, value\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, cause\u001b[39m=\u001b[39me, type_override\u001b[39m=\u001b[39mConfigAttributeError\n\u001b[1;32m    357\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/microscopy/lib/python3.9/site-packages/omegaconf/dictconfig.py:451\u001b[0m, in \u001b[0;36mDictConfig._get_impl\u001b[0;34m(self, key, default_value, validate_key)\u001b[0m\n\u001b[1;32m    449\u001b[0m         \u001b[39mraise\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(node, Node)\n\u001b[0;32m--> 451\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_resolve_with_default(\n\u001b[1;32m    452\u001b[0m     key\u001b[39m=\u001b[39;49mkey, value\u001b[39m=\u001b[39;49mnode, default_value\u001b[39m=\u001b[39;49mdefault_value\n\u001b[1;32m    453\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/microscopy/lib/python3.9/site-packages/omegaconf/basecontainer.py:98\u001b[0m, in \u001b[0;36mBaseContainer._resolve_with_default\u001b[0;34m(self, key, value, default_value)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[39mreturn\u001b[39;00m default_value\n\u001b[1;32m     96\u001b[0m     \u001b[39mraise\u001b[39;00m MissingMandatoryValue(\u001b[39m\"\u001b[39m\u001b[39mMissing mandatory value: $FULL_KEY\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 98\u001b[0m resolved_node \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_resolve_interpolation(\n\u001b[1;32m     99\u001b[0m     parent\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m    100\u001b[0m     key\u001b[39m=\u001b[39;49mkey,\n\u001b[1;32m    101\u001b[0m     value\u001b[39m=\u001b[39;49mvalue,\n\u001b[1;32m    102\u001b[0m     throw_on_resolution_failure\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    103\u001b[0m )\n\u001b[1;32m    105\u001b[0m \u001b[39mreturn\u001b[39;00m _get_value(resolved_node)\n",
      "File \u001b[0;32m~/miniconda3/envs/microscopy/lib/python3.9/site-packages/omegaconf/base.py:719\u001b[0m, in \u001b[0;36mContainer._maybe_resolve_interpolation\u001b[0;34m(self, parent, key, value, throw_on_resolution_failure, memo)\u001b[0m\n\u001b[1;32m    716\u001b[0m     \u001b[39mreturn\u001b[39;00m value\n\u001b[1;32m    718\u001b[0m parse_tree \u001b[39m=\u001b[39m parse(_get_value(value))\n\u001b[0;32m--> 719\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_resolve_interpolation_from_parse_tree(\n\u001b[1;32m    720\u001b[0m     parent\u001b[39m=\u001b[39;49mparent,\n\u001b[1;32m    721\u001b[0m     value\u001b[39m=\u001b[39;49mvalue,\n\u001b[1;32m    722\u001b[0m     key\u001b[39m=\u001b[39;49mkey,\n\u001b[1;32m    723\u001b[0m     parse_tree\u001b[39m=\u001b[39;49mparse_tree,\n\u001b[1;32m    724\u001b[0m     throw_on_resolution_failure\u001b[39m=\u001b[39;49mthrow_on_resolution_failure,\n\u001b[1;32m    725\u001b[0m     memo\u001b[39m=\u001b[39;49mmemo \u001b[39mif\u001b[39;49;00m memo \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mset\u001b[39;49m(),\n\u001b[1;32m    726\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/microscopy/lib/python3.9/site-packages/omegaconf/base.py:584\u001b[0m, in \u001b[0;36mContainer._resolve_interpolation_from_parse_tree\u001b[0;34m(self, parent, value, key, parse_tree, throw_on_resolution_failure, memo)\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    555\u001b[0m \u001b[39mResolve an interpolation.\u001b[39;00m\n\u001b[1;32m    556\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    580\u001b[0m \u001b[39m    `throw_on_resolution_failure` is `False` and an error occurs during resolution.\u001b[39;00m\n\u001b[1;32m    581\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    583\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 584\u001b[0m     resolved \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mresolve_parse_tree(\n\u001b[1;32m    585\u001b[0m         parse_tree\u001b[39m=\u001b[39;49mparse_tree, node\u001b[39m=\u001b[39;49mvalue, key\u001b[39m=\u001b[39;49mkey, memo\u001b[39m=\u001b[39;49mmemo\n\u001b[1;32m    586\u001b[0m     )\n\u001b[1;32m    587\u001b[0m \u001b[39mexcept\u001b[39;00m InterpolationResolutionError:\n\u001b[1;32m    588\u001b[0m     \u001b[39mif\u001b[39;00m throw_on_resolution_failure:\n",
      "File \u001b[0;32m~/miniconda3/envs/microscopy/lib/python3.9/site-packages/omegaconf/base.py:764\u001b[0m, in \u001b[0;36mContainer.resolve_parse_tree\u001b[0;34m(self, parse_tree, node, memo, key)\u001b[0m\n\u001b[1;32m    758\u001b[0m visitor \u001b[39m=\u001b[39m GrammarVisitor(\n\u001b[1;32m    759\u001b[0m     node_interpolation_callback\u001b[39m=\u001b[39mnode_interpolation_callback,\n\u001b[1;32m    760\u001b[0m     resolver_interpolation_callback\u001b[39m=\u001b[39mresolver_interpolation_callback,\n\u001b[1;32m    761\u001b[0m     memo\u001b[39m=\u001b[39mmemo,\n\u001b[1;32m    762\u001b[0m )\n\u001b[1;32m    763\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 764\u001b[0m     \u001b[39mreturn\u001b[39;00m visitor\u001b[39m.\u001b[39;49mvisit(parse_tree)\n\u001b[1;32m    765\u001b[0m \u001b[39mexcept\u001b[39;00m InterpolationResolutionError:\n\u001b[1;32m    766\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/microscopy/lib/python3.9/site-packages/antlr4/tree/Tree.py:34\u001b[0m, in \u001b[0;36mParseTreeVisitor.visit\u001b[0;34m(self, tree)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvisit\u001b[39m(\u001b[39mself\u001b[39m, tree):\n\u001b[0;32m---> 34\u001b[0m     \u001b[39mreturn\u001b[39;00m tree\u001b[39m.\u001b[39;49maccept(\u001b[39mself\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/microscopy/lib/python3.9/site-packages/omegaconf/grammar/gen/OmegaConfGrammarParser.py:206\u001b[0m, in \u001b[0;36mOmegaConfGrammarParser.ConfigValueContext.accept\u001b[0;34m(self, visitor)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39maccept\u001b[39m(\u001b[39mself\u001b[39m, visitor:ParseTreeVisitor):\n\u001b[1;32m    205\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m( visitor, \u001b[39m\"\u001b[39m\u001b[39mvisitConfigValue\u001b[39m\u001b[39m\"\u001b[39m ):\n\u001b[0;32m--> 206\u001b[0m         \u001b[39mreturn\u001b[39;00m visitor\u001b[39m.\u001b[39;49mvisitConfigValue(\u001b[39mself\u001b[39;49m)\n\u001b[1;32m    207\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m         \u001b[39mreturn\u001b[39;00m visitor\u001b[39m.\u001b[39mvisitChildren(\u001b[39mself\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/microscopy/lib/python3.9/site-packages/omegaconf/grammar_visitor.py:101\u001b[0m, in \u001b[0;36mGrammarVisitor.visitConfigValue\u001b[0;34m(self, ctx)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvisitConfigValue\u001b[39m(\u001b[39mself\u001b[39m, ctx: OmegaConfGrammarParser\u001b[39m.\u001b[39mConfigValueContext) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m     99\u001b[0m     \u001b[39m# text EOF\u001b[39;00m\n\u001b[1;32m    100\u001b[0m     \u001b[39massert\u001b[39;00m ctx\u001b[39m.\u001b[39mgetChildCount() \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m\n\u001b[0;32m--> 101\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvisit(ctx\u001b[39m.\u001b[39;49mgetChild(\u001b[39m0\u001b[39;49m))\n",
      "File \u001b[0;32m~/miniconda3/envs/microscopy/lib/python3.9/site-packages/antlr4/tree/Tree.py:34\u001b[0m, in \u001b[0;36mParseTreeVisitor.visit\u001b[0;34m(self, tree)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvisit\u001b[39m(\u001b[39mself\u001b[39m, tree):\n\u001b[0;32m---> 34\u001b[0m     \u001b[39mreturn\u001b[39;00m tree\u001b[39m.\u001b[39;49maccept(\u001b[39mself\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/microscopy/lib/python3.9/site-packages/omegaconf/grammar/gen/OmegaConfGrammarParser.py:342\u001b[0m, in \u001b[0;36mOmegaConfGrammarParser.TextContext.accept\u001b[0;34m(self, visitor)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39maccept\u001b[39m(\u001b[39mself\u001b[39m, visitor:ParseTreeVisitor):\n\u001b[1;32m    341\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m( visitor, \u001b[39m\"\u001b[39m\u001b[39mvisitText\u001b[39m\u001b[39m\"\u001b[39m ):\n\u001b[0;32m--> 342\u001b[0m         \u001b[39mreturn\u001b[39;00m visitor\u001b[39m.\u001b[39;49mvisitText(\u001b[39mself\u001b[39;49m)\n\u001b[1;32m    343\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    344\u001b[0m         \u001b[39mreturn\u001b[39;00m visitor\u001b[39m.\u001b[39mvisitChildren(\u001b[39mself\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/microscopy/lib/python3.9/site-packages/omegaconf/grammar_visitor.py:298\u001b[0m, in \u001b[0;36mGrammarVisitor.visitText\u001b[0;34m(self, ctx)\u001b[0m\n\u001b[1;32m    296\u001b[0m     c \u001b[39m=\u001b[39m ctx\u001b[39m.\u001b[39mgetChild(\u001b[39m0\u001b[39m)\n\u001b[1;32m    297\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(c, OmegaConfGrammarParser\u001b[39m.\u001b[39mInterpolationContext):\n\u001b[0;32m--> 298\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvisitInterpolation(c)\n\u001b[1;32m    300\u001b[0m \u001b[39m# Otherwise, concatenate string representations together.\u001b[39;00m\n\u001b[1;32m    301\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_unescape(\u001b[39mlist\u001b[39m(ctx\u001b[39m.\u001b[39mgetChildren()))\n",
      "File \u001b[0;32m~/miniconda3/envs/microscopy/lib/python3.9/site-packages/omegaconf/grammar_visitor.py:125\u001b[0m, in \u001b[0;36mGrammarVisitor.visitInterpolation\u001b[0;34m(self, ctx)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvisitInterpolation\u001b[39m(\n\u001b[1;32m    122\u001b[0m     \u001b[39mself\u001b[39m, ctx: OmegaConfGrammarParser\u001b[39m.\u001b[39mInterpolationContext\n\u001b[1;32m    123\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    124\u001b[0m     \u001b[39massert\u001b[39;00m ctx\u001b[39m.\u001b[39mgetChildCount() \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m  \u001b[39m# interpolationNode | interpolationResolver\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvisit(ctx\u001b[39m.\u001b[39;49mgetChild(\u001b[39m0\u001b[39;49m))\n",
      "File \u001b[0;32m~/miniconda3/envs/microscopy/lib/python3.9/site-packages/antlr4/tree/Tree.py:34\u001b[0m, in \u001b[0;36mParseTreeVisitor.visit\u001b[0;34m(self, tree)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvisit\u001b[39m(\u001b[39mself\u001b[39m, tree):\n\u001b[0;32m---> 34\u001b[0m     \u001b[39mreturn\u001b[39;00m tree\u001b[39m.\u001b[39;49maccept(\u001b[39mself\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/microscopy/lib/python3.9/site-packages/omegaconf/grammar/gen/OmegaConfGrammarParser.py:921\u001b[0m, in \u001b[0;36mOmegaConfGrammarParser.InterpolationNodeContext.accept\u001b[0;34m(self, visitor)\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39maccept\u001b[39m(\u001b[39mself\u001b[39m, visitor:ParseTreeVisitor):\n\u001b[1;32m    920\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m( visitor, \u001b[39m\"\u001b[39m\u001b[39mvisitInterpolationNode\u001b[39m\u001b[39m\"\u001b[39m ):\n\u001b[0;32m--> 921\u001b[0m         \u001b[39mreturn\u001b[39;00m visitor\u001b[39m.\u001b[39;49mvisitInterpolationNode(\u001b[39mself\u001b[39;49m)\n\u001b[1;32m    922\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    923\u001b[0m         \u001b[39mreturn\u001b[39;00m visitor\u001b[39m.\u001b[39mvisitChildren(\u001b[39mself\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/microscopy/lib/python3.9/site-packages/omegaconf/grammar_visitor.py:158\u001b[0m, in \u001b[0;36mGrammarVisitor.visitInterpolationNode\u001b[0;34m(self, ctx)\u001b[0m\n\u001b[1;32m    155\u001b[0m         inter_key_tokens\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvisitConfigKey(child))\n\u001b[1;32m    157\u001b[0m inter_key \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(inter_key_tokens)\n\u001b[0;32m--> 158\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnode_interpolation_callback(inter_key, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmemo)\n",
      "File \u001b[0;32m~/miniconda3/envs/microscopy/lib/python3.9/site-packages/omegaconf/base.py:745\u001b[0m, in \u001b[0;36mContainer.resolve_parse_tree.<locals>.node_interpolation_callback\u001b[0;34m(inter_key, memo)\u001b[0m\n\u001b[1;32m    742\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnode_interpolation_callback\u001b[39m(\n\u001b[1;32m    743\u001b[0m     inter_key: \u001b[39mstr\u001b[39m, memo: Optional[Set[\u001b[39mint\u001b[39m]]\n\u001b[1;32m    744\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Optional[\u001b[39m\"\u001b[39m\u001b[39mNode\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m--> 745\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_resolve_node_interpolation(inter_key\u001b[39m=\u001b[39;49minter_key, memo\u001b[39m=\u001b[39;49mmemo)\n",
      "File \u001b[0;32m~/miniconda3/envs/microscopy/lib/python3.9/site-packages/omegaconf/base.py:676\u001b[0m, in \u001b[0;36mContainer._resolve_node_interpolation\u001b[0;34m(self, inter_key, memo)\u001b[0m\n\u001b[1;32m    671\u001b[0m     \u001b[39mraise\u001b[39;00m InterpolationToMissingValueError(\n\u001b[1;32m    672\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMissingMandatoryValue while resolving interpolation: \u001b[39m\u001b[39m{\u001b[39;00mexc\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    673\u001b[0m     )\u001b[39m.\u001b[39mwith_traceback(sys\u001b[39m.\u001b[39mexc_info()[\u001b[39m2\u001b[39m])\n\u001b[1;32m    675\u001b[0m \u001b[39mif\u001b[39;00m parent \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m value \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 676\u001b[0m     \u001b[39mraise\u001b[39;00m InterpolationKeyError(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInterpolation key \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00minter_key\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m not found\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    677\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    678\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_not_dereferencing_to_parent(node\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, target\u001b[39m=\u001b[39mvalue)\n",
      "\u001b[0;31mInterpolationKeyError\u001b[0m: Interpolation key 'dataset.F-actin' not found\n    full_key: used_dataset\n    object_type=dict"
     ]
    }
   ],
   "source": [
    "def load_path(dataset_root, dataset_name, folder):\n",
    "    if folder is not None:\n",
    "        return os.path.join(dataset_root, dataset_name, folder)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def my_app(cfg: DictConfig) -> None:\n",
    "    print(cfg)\n",
    "    #\"LiveFActinDataset\", \"EM\", \"MitoTracker_small\", \"F-actin\", \"ER\", \"MT\", \"MT-SMLM_all\"\n",
    "    for dataset_name in [\"F-actin\"]:  \n",
    "        cfg.dataset_name = dataset_name\n",
    "        train_lr, train_hr, val_lr, val_hr, test_lr, test_hr = cfg.used_dataset.data_paths\n",
    "\n",
    "        dataset_root = \"datasets\" if os.path.exists(\"datasets\") else \"../datasets\"\n",
    "        train_lr_path = load_path(dataset_root, dataset_name, train_lr)\n",
    "        train_hr_path = load_path(dataset_root, dataset_name, train_hr)\n",
    "        val_lr_path = load_path(dataset_root, dataset_name, val_lr)\n",
    "        val_hr_path = load_path(dataset_root, dataset_name, val_hr)\n",
    "        test_lr_path = load_path(dataset_root, dataset_name, test_lr)\n",
    "        test_hr_path = load_path(dataset_root, dataset_name, test_hr)\n",
    "\n",
    "        # \"unet\", \"rcan\", \"dfcan\", \"wdsr\", \"wgan\", \"esrganplus\", \"cddpm\"\n",
    "        model_name = 'cddpm'\n",
    "        batch_size = 4\n",
    "        num_epochs = 5\n",
    "        lr = 0.001\n",
    "        discriminator_lr = 0.001\n",
    "        scheduler = 'ReduceOnPlateau'\n",
    "        optimizer = 'adam'\n",
    "        \n",
    "        cfg.model_name = model_name\n",
    "        cfg.hyperparam.batch_size = batch_size\n",
    "        cfg.hyperparam.num_epochs = num_epochs\n",
    "        cfg.hyperparam.lr = lr\n",
    "        cfg.hyperparam.discriminator_lr = discriminator_lr\n",
    "\n",
    "        cfg.hyperparam.scheduler = scheduler\n",
    "        cfg.hyperparam.discriminator_lr_scheduler = scheduler\n",
    "        cfg.hyperparam.optimizer = optimizer\n",
    "        cfg.hyperparam.discriminator_optimizer = optimizer\n",
    "\n",
    "        cfg.model.optim.early_stop.patience = num_epochs\n",
    "        save_folder = \"scale\" + str(cfg.used_dataset.scale)\n",
    "        if cfg.hyperparam.additional_folder is not None:\n",
    "            save_folder += \"_\" + cfg.hyperparam.additional_folder\n",
    "\n",
    "        saving_path = \"./results/{}/{}/{}/epc{}_btch{}_lr{}_optim-{}_lrsched-{}_seed{}\".format(\n",
    "            cfg.dataset_name,\n",
    "            cfg.model_name,\n",
    "            save_folder,\n",
    "            cfg.hyperparam.num_epochs,\n",
    "            cfg.hyperparam.batch_size,\n",
    "            cfg.hyperparam.lr,\n",
    "            cfg.hyperparam.optimizer,\n",
    "            cfg.hyperparam.scheduler,\n",
    "            cfg.hyperparam.seed\n",
    "        )\n",
    "\n",
    "        return cfg, train_lr_path, train_hr_path, val_lr_path, val_hr_path, test_lr_path, test_hr_path, saving_path, 1\n",
    "\n",
    "cfg = cfg = OmegaConf.load('../conf/config.yaml')\n",
    "cfg, train_lr_path, train_hr_path, val_lr_path, val_hr_path, test_lr_path, test_hr_path, saving_path, verbose = my_app(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [--help] [--hydra-help] [--version]\n",
      "                             [--cfg {job,hydra,all}] [--resolve]\n",
      "                             [--package PACKAGE] [--run] [--multirun]\n",
      "                             [--shell-completion] [--config-path CONFIG_PATH]\n",
      "                             [--config-name CONFIG_NAME]\n",
      "                             [--config-dir CONFIG_DIR]\n",
      "                             [--experimental-rerun EXPERIMENTAL_RERUN]\n",
      "                             [--info [{all,config,defaults,defaults-tree,plugins,searchpath}]]\n",
      "                             [overrides ...]\n",
      "ipykernel_launcher.py: error: argument --shell-completion/-sc: ignored explicit argument '9002'\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'tb_frame'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArgumentError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/microscopy/lib/python3.9/argparse.py:1858\u001b[0m, in \u001b[0;36mArgumentParser.parse_known_args\u001b[0;34m(self, args, namespace)\u001b[0m\n\u001b[1;32m   1857\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1858\u001b[0m     namespace, args \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_parse_known_args(args, namespace)\n\u001b[1;32m   1859\u001b[0m \u001b[39mexcept\u001b[39;00m ArgumentError:\n",
      "File \u001b[0;32m~/miniconda3/envs/microscopy/lib/python3.9/argparse.py:2067\u001b[0m, in \u001b[0;36mArgumentParser._parse_known_args\u001b[0;34m(self, arg_strings, namespace)\u001b[0m\n\u001b[1;32m   2066\u001b[0m     \u001b[39m# consume the next optional and any arguments for it\u001b[39;00m\n\u001b[0;32m-> 2067\u001b[0m     start_index \u001b[39m=\u001b[39m consume_optional(start_index)\n\u001b[1;32m   2069\u001b[0m \u001b[39m# consume any positionals following the last Optional\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/microscopy/lib/python3.9/argparse.py:1989\u001b[0m, in \u001b[0;36mArgumentParser._parse_known_args.<locals>.consume_optional\u001b[0;34m(start_index)\u001b[0m\n\u001b[1;32m   1988\u001b[0m         msg \u001b[39m=\u001b[39m _(\u001b[39m'\u001b[39m\u001b[39mignored explicit argument \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m-> 1989\u001b[0m         \u001b[39mraise\u001b[39;00m ArgumentError(action, msg \u001b[39m%\u001b[39m explicit_arg)\n\u001b[1;32m   1991\u001b[0m \u001b[39m# if there is no explicit argument, try to match the\u001b[39;00m\n\u001b[1;32m   1992\u001b[0m \u001b[39m# optional's string arguments with the following strings\u001b[39;00m\n\u001b[1;32m   1993\u001b[0m \u001b[39m# if successful, exit the loop\u001b[39;00m\n\u001b[1;32m   1994\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mArgumentError\u001b[0m: argument --shell-completion/-sc: ignored explicit argument '9002'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m cfg, train_lr_path, train_hr_path, val_lr_path, val_hr_path, test_lr_path, test_hr_path, saving_path, verbose \u001b[39m=\u001b[39m my_app()\n",
      "File \u001b[0;32m~/miniconda3/envs/microscopy/lib/python3.9/site-packages/hydra/main.py:86\u001b[0m, in \u001b[0;36mmain.<locals>.main_decorator.<locals>.decorated_main\u001b[0;34m(cfg_passthrough)\u001b[0m\n\u001b[1;32m     85\u001b[0m args_parser \u001b[39m=\u001b[39m get_args_parser()\n\u001b[0;32m---> 86\u001b[0m args \u001b[39m=\u001b[39m args_parser\u001b[39m.\u001b[39;49mparse_args()\n\u001b[1;32m     87\u001b[0m \u001b[39mif\u001b[39;00m args\u001b[39m.\u001b[39mexperimental_rerun \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/microscopy/lib/python3.9/argparse.py:1825\u001b[0m, in \u001b[0;36mArgumentParser.parse_args\u001b[0;34m(self, args, namespace)\u001b[0m\n\u001b[1;32m   1824\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mparse_args\u001b[39m(\u001b[39mself\u001b[39m, args\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, namespace\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m-> 1825\u001b[0m     args, argv \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparse_known_args(args, namespace)\n\u001b[1;32m   1826\u001b[0m     \u001b[39mif\u001b[39;00m argv:\n",
      "File \u001b[0;32m~/miniconda3/envs/microscopy/lib/python3.9/argparse.py:1861\u001b[0m, in \u001b[0;36mArgumentParser.parse_known_args\u001b[0;34m(self, args, namespace)\u001b[0m\n\u001b[1;32m   1860\u001b[0m         err \u001b[39m=\u001b[39m _sys\u001b[39m.\u001b[39mexc_info()[\u001b[39m1\u001b[39m]\n\u001b[0;32m-> 1861\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merror(\u001b[39mstr\u001b[39;49m(err))\n\u001b[1;32m   1862\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/microscopy/lib/python3.9/argparse.py:2582\u001b[0m, in \u001b[0;36mArgumentParser.error\u001b[0;34m(self, message)\u001b[0m\n\u001b[1;32m   2581\u001b[0m args \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mprog\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprog, \u001b[39m'\u001b[39m\u001b[39mmessage\u001b[39m\u001b[39m'\u001b[39m: message}\n\u001b[0;32m-> 2582\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexit(\u001b[39m2\u001b[39;49m, _(\u001b[39m'\u001b[39;49m\u001b[39m%(prog)s\u001b[39;49;00m\u001b[39m: error: \u001b[39;49m\u001b[39m%(message)s\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m'\u001b[39;49m) \u001b[39m%\u001b[39;49m args)\n",
      "File \u001b[0;32m~/miniconda3/envs/microscopy/lib/python3.9/argparse.py:2569\u001b[0m, in \u001b[0;36mArgumentParser.exit\u001b[0;34m(self, status, message)\u001b[0m\n\u001b[1;32m   2568\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_print_message(message, _sys\u001b[39m.\u001b[39mstderr)\n\u001b[0;32m-> 2569\u001b[0m _sys\u001b[39m.\u001b[39;49mexit(status)\n",
      "\u001b[0;31mSystemExit\u001b[0m: 2",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/microscopy/lib/python3.9/site-packages/IPython/core/interactiveshell.py:2095\u001b[0m, in \u001b[0;36mInteractiveShell.showtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2092\u001b[0m \u001b[39mif\u001b[39;00m exception_only:\n\u001b[1;32m   2093\u001b[0m     stb \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mAn exception has occurred, use \u001b[39m\u001b[39m%\u001b[39m\u001b[39mtb to see \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   2094\u001b[0m            \u001b[39m'\u001b[39m\u001b[39mthe full traceback.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m]\n\u001b[0;32m-> 2095\u001b[0m     stb\u001b[39m.\u001b[39mextend(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mInteractiveTB\u001b[39m.\u001b[39;49mget_exception_only(etype,\n\u001b[1;32m   2096\u001b[0m                                                      value))\n\u001b[1;32m   2097\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2098\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   2099\u001b[0m         \u001b[39m# Exception classes can customise their traceback - we\u001b[39;00m\n\u001b[1;32m   2100\u001b[0m         \u001b[39m# use this in IPython.parallel for exceptions occurring\u001b[39;00m\n\u001b[1;32m   2101\u001b[0m         \u001b[39m# in the engines. This should return a list of strings.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/microscopy/lib/python3.9/site-packages/IPython/core/ultratb.py:710\u001b[0m, in \u001b[0;36mListTB.get_exception_only\u001b[0;34m(self, etype, value)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_exception_only\u001b[39m(\u001b[39mself\u001b[39m, etype, value):\n\u001b[1;32m    703\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Only print the exception type and message, without a traceback.\u001b[39;00m\n\u001b[1;32m    704\u001b[0m \n\u001b[1;32m    705\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    708\u001b[0m \u001b[39m    value : exception value\u001b[39;00m\n\u001b[1;32m    709\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 710\u001b[0m     \u001b[39mreturn\u001b[39;00m ListTB\u001b[39m.\u001b[39;49mstructured_traceback(\u001b[39mself\u001b[39;49m, etype, value)\n",
      "File \u001b[0;32m~/miniconda3/envs/microscopy/lib/python3.9/site-packages/IPython/core/ultratb.py:568\u001b[0m, in \u001b[0;36mListTB.structured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, context)\u001b[0m\n\u001b[1;32m    565\u001b[0m     chained_exc_ids\u001b[39m.\u001b[39madd(\u001b[39mid\u001b[39m(exception[\u001b[39m1\u001b[39m]))\n\u001b[1;32m    566\u001b[0m     chained_exceptions_tb_offset \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    567\u001b[0m     out_list \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 568\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstructured_traceback(\n\u001b[1;32m    569\u001b[0m             etype,\n\u001b[1;32m    570\u001b[0m             evalue,\n\u001b[1;32m    571\u001b[0m             (etb, chained_exc_ids),  \u001b[39m# type: ignore\u001b[39;49;00m\n\u001b[1;32m    572\u001b[0m             chained_exceptions_tb_offset,\n\u001b[1;32m    573\u001b[0m             context,\n\u001b[1;32m    574\u001b[0m         )\n\u001b[1;32m    575\u001b[0m         \u001b[39m+\u001b[39m chained_exception_message\n\u001b[1;32m    576\u001b[0m         \u001b[39m+\u001b[39m out_list)\n\u001b[1;32m    578\u001b[0m \u001b[39mreturn\u001b[39;00m out_list\n",
      "File \u001b[0;32m~/miniconda3/envs/microscopy/lib/python3.9/site-packages/IPython/core/ultratb.py:1428\u001b[0m, in \u001b[0;36mAutoFormattedTB.structured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1426\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1427\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtb \u001b[39m=\u001b[39m etb\n\u001b[0;32m-> 1428\u001b[0m \u001b[39mreturn\u001b[39;00m FormattedTB\u001b[39m.\u001b[39;49mstructured_traceback(\n\u001b[1;32m   1429\u001b[0m     \u001b[39mself\u001b[39;49m, etype, evalue, etb, tb_offset, number_of_lines_of_context\n\u001b[1;32m   1430\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/microscopy/lib/python3.9/site-packages/IPython/core/ultratb.py:1319\u001b[0m, in \u001b[0;36mFormattedTB.structured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1316\u001b[0m mode \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode\n\u001b[1;32m   1317\u001b[0m \u001b[39mif\u001b[39;00m mode \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose_modes:\n\u001b[1;32m   1318\u001b[0m     \u001b[39m# Verbose modes need a full traceback\u001b[39;00m\n\u001b[0;32m-> 1319\u001b[0m     \u001b[39mreturn\u001b[39;00m VerboseTB\u001b[39m.\u001b[39;49mstructured_traceback(\n\u001b[1;32m   1320\u001b[0m         \u001b[39mself\u001b[39;49m, etype, value, tb, tb_offset, number_of_lines_of_context\n\u001b[1;32m   1321\u001b[0m     )\n\u001b[1;32m   1322\u001b[0m \u001b[39melif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mMinimal\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m   1323\u001b[0m     \u001b[39mreturn\u001b[39;00m ListTB\u001b[39m.\u001b[39mget_exception_only(\u001b[39mself\u001b[39m, etype, value)\n",
      "File \u001b[0;32m~/miniconda3/envs/microscopy/lib/python3.9/site-packages/IPython/core/ultratb.py:1172\u001b[0m, in \u001b[0;36mVerboseTB.structured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1163\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstructured_traceback\u001b[39m(\n\u001b[1;32m   1164\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1165\u001b[0m     etype: \u001b[39mtype\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1169\u001b[0m     number_of_lines_of_context: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m5\u001b[39m,\n\u001b[1;32m   1170\u001b[0m ):\n\u001b[1;32m   1171\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1172\u001b[0m     formatted_exception \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mformat_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[1;32m   1173\u001b[0m                                                            tb_offset)\n\u001b[1;32m   1175\u001b[0m     colors \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mColors  \u001b[39m# just a shorthand + quicker name lookup\u001b[39;00m\n\u001b[1;32m   1176\u001b[0m     colorsnormal \u001b[39m=\u001b[39m colors\u001b[39m.\u001b[39mNormal  \u001b[39m# used a lot\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/microscopy/lib/python3.9/site-packages/IPython/core/ultratb.py:1062\u001b[0m, in \u001b[0;36mVerboseTB.format_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1059\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(tb_offset, \u001b[39mint\u001b[39m)\n\u001b[1;32m   1060\u001b[0m head \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_header(\u001b[39mstr\u001b[39m(etype), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlong_header)\n\u001b[1;32m   1061\u001b[0m records \u001b[39m=\u001b[39m (\n\u001b[0;32m-> 1062\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_records(etb, number_of_lines_of_context, tb_offset) \u001b[39mif\u001b[39;00m etb \u001b[39melse\u001b[39;00m []\n\u001b[1;32m   1063\u001b[0m )\n\u001b[1;32m   1065\u001b[0m frames \u001b[39m=\u001b[39m []\n\u001b[1;32m   1066\u001b[0m skipped \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/microscopy/lib/python3.9/site-packages/IPython/core/ultratb.py:1130\u001b[0m, in \u001b[0;36mVerboseTB.get_records\u001b[0;34m(self, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1128\u001b[0m \u001b[39mwhile\u001b[39;00m cf \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1129\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1130\u001b[0m         mod \u001b[39m=\u001b[39m inspect\u001b[39m.\u001b[39mgetmodule(cf\u001b[39m.\u001b[39;49mtb_frame)\n\u001b[1;32m   1131\u001b[0m         \u001b[39mif\u001b[39;00m mod \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1132\u001b[0m             mod_name \u001b[39m=\u001b[39m mod\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'tb_frame'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "372\n",
      "372\n",
      "60\n",
      "60\n",
      "15\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "data_name = 'F-actin'\n",
    "\n",
    "train_lr_path = '../../datasets/F-actin/train/training_wf'\n",
    "train_hr_path = '../../datasets/F-actin/train/training_gt'\n",
    "val_lr_path = '../../datasets/F-actin/val/validate_wf'\n",
    "val_hr_path = '../../datasets/F-actin/val/validate_gt'\n",
    "test_lr_path = '../../datasets/F-actin/test/test_wf/level_01'\n",
    "test_hr_path = '../../datasets/F-actin/test/test_gt'\n",
    "\n",
    "train_extension_list = [\n",
    "    os.path.splitext(e)[1] for e in os.listdir(train_hr_path)\n",
    "]\n",
    "train_extension = max(set(train_extension_list), key=train_extension_list.count)\n",
    "train_filenames = sorted(\n",
    "    [x for x in os.listdir(train_hr_path) if x.endswith(train_extension)]\n",
    ")\n",
    "\n",
    "validation_split = 0.1\n",
    "if val_hr_path is None or val_lr_path is None:\n",
    "    val_lr_path = train_lr_path\n",
    "    val_hr_path = train_hr_path\n",
    "\n",
    "    val_filenames = train_filenames[\n",
    "        int(len(train_filenames) * (1 - validation_split )) :\n",
    "    ]\n",
    "    train_filenames = train_filenames[\n",
    "        : int(len(train_filenames) * (1 - validation_split))\n",
    "    ]\n",
    "else:\n",
    "    val_lr_path = val_lr_path\n",
    "    val_hr_path = val_hr_path\n",
    "\n",
    "    val_extension_list = [\n",
    "        os.path.splitext(e)[1] for e in os.listdir(val_hr_path)\n",
    "    ]\n",
    "    val_extension = max(set(val_extension_list), key=val_extension_list.count)\n",
    "    val_filenames = sorted(\n",
    "        [x for x in os.listdir(val_hr_path) if x.endswith(val_extension)]\n",
    "    )\n",
    "\n",
    "test_lr_path = test_lr_path\n",
    "test_hr_path = test_hr_path\n",
    "test_extension_list = [\n",
    "    os.path.splitext(e)[1] for e in os.listdir(test_hr_path)\n",
    "]\n",
    "test_extension = max(set(test_extension_list), key=test_extension_list.count)\n",
    "test_filenames = sorted(\n",
    "    [x for x in os.listdir(test_hr_path) if x.endswith(test_extension)]\n",
    ")\n",
    "\n",
    "print(len((os.listdir(train_hr_path))))\n",
    "print(len((os.listdir(train_lr_path))))\n",
    "print(len((os.listdir(val_hr_path))))\n",
    "print(len((os.listdir(val_lr_path))))\n",
    "print(len((os.listdir(test_hr_path))))\n",
    "print(len((os.listdir(test_lr_path))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crappifier_method = 'downsampleonly'\n",
    "scale_factor = None\n",
    "lr_patch_size_x = 64\n",
    "lr_patch_size_y = 64\n",
    "datagen_sampling_pdf = 0\n",
    "\n",
    "rotation = True\n",
    "horizontal_flip = True\n",
    "vertical_flip = True\n",
    "\n",
    "model_name = 'cddpm'\n",
    "num_epochs = 3\n",
    "batch_size = 4\n",
    "learning_rate = 0.001\n",
    "discriminator_learning_rate = 0.001\n",
    "optimizer_name = 'OneCycle'\n",
    "discriminator_optimizer = 'OneCycle'\n",
    "lr_scheduler_name = 'adam'\n",
    "discriminator_lr_scheduler = 'adam'\n",
    "\n",
    "test_metric_indexes = [1,2,3,4,5]\n",
    "\n",
    "additional_folder = ''\n",
    "seed = 666\n",
    "\n",
    "verbose = True\n",
    "\n",
    "utils.set_seed(seed)\n",
    "\n",
    "save_folder = \"scale\" + str(scale_factor)\n",
    "\n",
    "if additional_folder:\n",
    "    save_folder += \"_\" + additional_folder\n",
    "\n",
    "saving_path = .\n",
    "\n",
    "os.makedirs(saving_path, exist_ok=True)\n",
    "utils.save_yaml(\n",
    "    config,\n",
    "    os.path.join(saving_path, \"train_configuration.yaml\"),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class ModelsTrainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config,\n",
    "        train_lr_path,\n",
    "        train_hr_path,\n",
    "        val_lr_path,\n",
    "        val_hr_path,\n",
    "        test_lr_path,\n",
    "        test_hr_path,\n",
    "        saving_path,\n",
    "        verbose=0,\n",
    "    ):\n",
    "\n",
    "\n",
    "        # To calculate the input and output shape and the actual scale factor \n",
    "\n",
    "        (\n",
    "            _,\n",
    "            train_input_shape,\n",
    "            train_output_shape,\n",
    "            actual_scale_factor,\n",
    "        ) = datasets.TFDataset(\n",
    "            filenames=self.train_filenames,\n",
    "            hr_data_path=self.train_hr_path,\n",
    "            lr_data_path=self.train_lr_path,\n",
    "            scale_factor=self.scale_factor,\n",
    "            crappifier_name=self.crappifier_method,\n",
    "            lr_patch_shape=(self.lr_patch_size_x, self.lr_patch_size_y),\n",
    "            datagen_sampling_pdf=self.datagen_sampling_pdf,\n",
    "            validation_split=0.1,\n",
    "            batch_size=self.batch_size,\n",
    "            rotation=self.rotation,\n",
    "            horizontal_flip=self.horizontal_flip,\n",
    "            vertical_flip=self.vertical_flip,\n",
    "        )\n",
    "\n",
    "        self.input_data_shape = train_input_shape\n",
    "        self.output_data_shape = train_output_shape\n",
    "\n",
    "        if self.scale_factor is None or self.scale_factor != actual_scale_factor:\n",
    "            self.scale_factor = actual_scale_factor\n",
    "            utils.update_yaml(\n",
    "                os.path.join(self.saving_path, \"train_configuration.yaml\"),\n",
    "                \"actual_scale_factor\",\n",
    "                actual_scale_factor,\n",
    "            )\n",
    "            if self.verbose:\n",
    "                print(\n",
    "                    \"Actual scale factor that will be used is: {}\".format(\n",
    "                        self.scale_factor\n",
    "                    )\n",
    "                )\n",
    "\n",
    "\n",
    "        print(\"\\n\" + \"-\" * 10)\n",
    "        print(\n",
    "            \"{} model will be trained with the next configuration\".format(\n",
    "                self.model_name\n",
    "            )\n",
    "        )\n",
    "        print(\"Dataset: {}\".format(self.data_name))\n",
    "        print(\"\\tTrain wf path: {}\".format(train_lr_path))\n",
    "        print(\"\\tTrain gt path: {}\".format(train_hr_path))\n",
    "        print(\"\\tVal wf path: {}\".format(val_lr_path))\n",
    "        print(\"\\tVal gt path: {}\".format(val_hr_path))\n",
    "        print(\"\\tTest wf path: {}\".format(test_lr_path))\n",
    "        print(\"\\tTest gt path: {}\".format(test_hr_path))\n",
    "        print(\"Preprocessing info:\")\n",
    "        print(\"\\tScale factor: {}\".format(self.scale_factor))\n",
    "        print(\"\\tCrappifier method: {}\".format(self.crappifier_method))\n",
    "        print(\"\\tPatch size: {} x {}\".format(self.lr_patch_size_x, self.lr_patch_size_y))\n",
    "        print(\"Training info:\")\n",
    "        print(\"\\tEpochs: {}\".format(self.num_epochs))\n",
    "        print(\"\\tBatchsize: {}\".format(self.batch_size))\n",
    "        print(\"\\tGen learning rate: {}\".format(self.learning_rate))\n",
    "        print(\"\\tDisc learning rate: {}\".format(self.discriminator_learning_rate))\n",
    "        print(\"\\tGen optimizer: {}\".format(self.optimizer_name))\n",
    "        print(\"\\tDisc optimizer: {}\".format(self.discriminator_optimizer))\n",
    "        print(\"\\tGen scheduler: {}\".format(self.lr_scheduler_name))\n",
    "        print(\"\\tDisc scheduler: {}\".format(self.discriminator_lr_scheduler))\n",
    "        print(\"-\" * 10)\n",
    "\n",
    "    def launch(self):\n",
    "        self.prepare_data()\n",
    "        self.train_model()\n",
    "        self.predict_images()\n",
    "        self.eval_model()\n",
    "\n",
    "        return self.history\n",
    "\n",
    "    def prepare_data(self):\n",
    "        raise NotImplementedError(\"prepare_data() not implemented.\")\n",
    "\n",
    "    def train_model(self):\n",
    "        raise NotImplementedError(\"train_model() not implemented.\")\n",
    "\n",
    "    def predict_images(self):\n",
    "        raise NotImplementedError(\"predict_images() not implemented\")\n",
    "\n",
    "    def eval_model(self):\n",
    "        if self.verbose:\n",
    "            utils.print_info(\"eval_model() - self.Y_test\", self.Y_test)\n",
    "            utils.print_info(\"eval_model() - self.predictions\", self.predictions)\n",
    "            utils.print_info(\"eval_model() - self.X_test\", self.X_test)\n",
    "\n",
    "        print(\"The predictions will be evaluated:\")\n",
    "        metrics_dict = metrics.obtain_metrics(\n",
    "            gt_image_list=self.Y_test,\n",
    "            predicted_image_list=self.predictions,\n",
    "            wf_image_list=self.X_test,\n",
    "            test_metric_indexes=self.test_metric_indexes,\n",
    "        )\n",
    "\n",
    "        os.makedirs(self.saving_path + \"/test_metrics\", exist_ok=True)\n",
    "\n",
    "        for key in metrics_dict.keys():\n",
    "            if len(metrics_dict[key]) > 0:\n",
    "                print(\"{}: {}\".format(key, np.mean(metrics_dict[key])))\n",
    "                np.save(\n",
    "                    self.saving_path + \"/test_metrics/\" + key + \".npy\",\n",
    "                    metrics_dict[key],\n",
    "                )\n",
    "\n",
    "\n",
    "class TensorflowTrainer(ModelsTrainer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        config,\n",
    "        train_lr_path,\n",
    "        train_hr_path,\n",
    "        val_lr_path,\n",
    "        val_hr_path,\n",
    "        test_lr_path,\n",
    "        test_hr_path,\n",
    "        saving_path,\n",
    "        verbose=0,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            config,\n",
    "            train_lr_path,\n",
    "            train_hr_path,\n",
    "            val_lr_path,\n",
    "            val_hr_path,\n",
    "            test_lr_path,\n",
    "            test_hr_path,\n",
    "            saving_path,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "\n",
    "        self.library_name = \"tensorflow\"\n",
    "\n",
    "    def prepare_data(self):\n",
    "        train_generator, train_input_shape,train_output_shape, actual_scale_factor = datasets.TFDataset(\n",
    "            filenames=self.train_filenames,\n",
    "            hr_data_path=self.train_hr_path,\n",
    "            lr_data_path=self.train_lr_path,\n",
    "            scale_factor=self.scale_factor,\n",
    "            crappifier_name=self.crappifier_method,\n",
    "            lr_patch_shape=(self.lr_patch_size_x, self.lr_patch_size_y),\n",
    "            datagen_sampling_pdf=self.datagen_sampling_pdf,\n",
    "            validation_split=0.1,\n",
    "            batch_size=self.batch_size,\n",
    "            rotation=self.rotation,\n",
    "            horizontal_flip=self.horizontal_flip,\n",
    "            vertical_flip=self.vertical_flip,\n",
    "        )\n",
    "\n",
    "        val_generator, _, _, _ = datasets.TFDataset(\n",
    "            filenames=self.val_filenames,\n",
    "            hr_data_path=self.val_hr_path,\n",
    "            lr_data_path=self.val_lr_path,\n",
    "            scale_factor=self.scale_factor,\n",
    "            crappifier_name=self.crappifier_method,\n",
    "            lr_patch_shape=(self.lr_patch_size_x, self.lr_patch_size_y),\n",
    "            datagen_sampling_pdf=self.datagen_sampling_pdf,\n",
    "            validation_split=0.1,\n",
    "            batch_size=self.batch_size,\n",
    "            rotation=self.rotation,\n",
    "            horizontal_flip=self.horizontal_flip,\n",
    "            vertical_flip=self.vertical_flip,\n",
    "        )\n",
    "\n",
    "        self.input_data_shape = train_input_shape\n",
    "        self.output_data_shape = train_output_shape\n",
    "\n",
    "        if self.verbose:\n",
    "            print(\"input_data_shape: {}\".format(self.input_data_shape))\n",
    "            print(\"output_data_shape: {}\".format(self.output_data_shape))\n",
    "\n",
    "        if self.scale_factor is None or self.scale_factor != actual_scale_factor:\n",
    "            self.scale_factor = actual_scale_factor\n",
    "            utils.update_yaml(\n",
    "                os.path.join(self.saving_path, \"train_configuration.yaml\"),\n",
    "                \"actual_scale_factor\",\n",
    "                actual_scale_factor,\n",
    "            )\n",
    "            if self.verbose:\n",
    "                print(\n",
    "                    \"Actual scale factor that will be used is: {}\".format(\n",
    "                        self.scale_factor\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        utils.update_yaml(\n",
    "            os.path.join(self.saving_path, \"train_configuration.yaml\"),\n",
    "            \"input_data_shape\",\n",
    "            self.input_data_shape,\n",
    "        )\n",
    "        utils.update_yaml(\n",
    "            os.path.join(self.saving_path, \"train_configuration.yaml\"),\n",
    "            \"output_data_shape\",\n",
    "            self.output_data_shape,\n",
    "        )\n",
    "\n",
    "        self.train_generator = train_generator\n",
    "        self.val_generator = val_generator\n",
    "\n",
    "    def train_model(self):\n",
    "\n",
    "        callbacks = []\n",
    "\n",
    "        lr_schedule = optimizer_scheduler_utils.select_lr_schedule(\n",
    "                    library_name=self.library_name,\n",
    "                    lr_scheduler_name=self.lr_scheduler_name,\n",
    "                    data_len=self.input_data_shape[0] // self.batch_size,\n",
    "                    num_epochs=self.num_epochs,\n",
    "                    learning_rate=self.learning_rate,\n",
    "                    monitor_loss='val_ssim_loss',\n",
    "                    name=None,\n",
    "                    optimizer=None,\n",
    "                    frequency=None,\n",
    "                    additional_configuration=self.config,\n",
    "        )\n",
    "\n",
    "        if self.lr_scheduler_name in [\"CosineDecay\", \"MultiStepScheduler\"]:\n",
    "            self.optim = optimizer_scheduler_utils.select_optimizer(\n",
    "                library_name=self.library_name,\n",
    "                optimizer_name=self.optimizer_name,\n",
    "                learning_rate=lr_schedule,\n",
    "                check_point=None,\n",
    "                parameters=None,\n",
    "                additional_configuration=self.config\n",
    "            )\n",
    "        else:\n",
    "            self.optim = optimizer_scheduler_utils.select_optimizer(\n",
    "                library_name=self.library_name,\n",
    "                optimizer_name=self.optimizer_name,\n",
    "                learning_rate=self.learning_rate,\n",
    "                check_point=None,\n",
    "                parameters=None,\n",
    "                additional_configuration=self.config\n",
    "            )\n",
    "            callbacks.append(lr_schedule)\n",
    "\n",
    "        model = model_utils.select_model(\n",
    "            model_name=self.model_name,\n",
    "            input_shape=self.input_data_shape,\n",
    "            output_channels=self.output_data_shape[-1],\n",
    "            scale_factor=self.scale_factor,\n",
    "            datagen_sampling_pdf=self.datagen_sampling_pdf,\n",
    "            model_configuration=self.config.used_model,\n",
    "            batch_size=self.batch_size,\n",
    "        )\n",
    "\n",
    "        loss_funct = tf.keras.losses.mean_absolute_error\n",
    "        eval_metric = tf.keras.losses.mean_squared_error\n",
    "\n",
    "        model.compile(\n",
    "            optimizer=self.optim,\n",
    "            loss=loss_funct,\n",
    "            metrics=[eval_metric, utils.ssim_loss],\n",
    "        )\n",
    "\n",
    "        trainableParams = np.sum(\n",
    "            [np.prod(v.get_shape()) for v in model.trainable_weights]\n",
    "        )\n",
    "        nonTrainableParams = np.sum(\n",
    "            [np.prod(v.get_shape()) for v in model.non_trainable_weights]\n",
    "        )\n",
    "        totalParams = trainableParams + nonTrainableParams\n",
    "\n",
    "        model_checkpoint = tf_ModelCheckpoint(\n",
    "            os.path.join(self.saving_path, \"weights_best.h5\"),\n",
    "            monitor=\"val_loss\",\n",
    "            verbose=1,\n",
    "            save_best_only=True,\n",
    "            save_weights_only=True,\n",
    "        )\n",
    "        callbacks.append(model_checkpoint)\n",
    "\n",
    "        # callback for early stopping\n",
    "        earlystopper = EarlyStopping(\n",
    "            monitor=self.config.model.optim.early_stop.loss,\n",
    "            patience=self.config.model.optim.early_stop.patience,\n",
    "            min_delta=0.005,\n",
    "            mode=self.config.model.optim.early_stop.mode,\n",
    "            verbose=1,\n",
    "            restore_best_weights=True,\n",
    "        )\n",
    "        callbacks.append(earlystopper)\n",
    "\n",
    "        for x, y in self.val_generator:\n",
    "            x_val = x\n",
    "            y_val = y\n",
    "            break\n",
    "\n",
    "        plt_saving_path = os.path.join(self.saving_path, \"training_images\")\n",
    "        os.makedirs(plt_saving_path, exist_ok=True)\n",
    "        plot_callback = tensorflow_callbacks.PerformancePlotCallback(\n",
    "            x_val, y_val, plt_saving_path, frequency=5, is_cddpm=self.model_name==\"cddpm\"\n",
    "        )\n",
    "        callbacks.append(plot_callback)\n",
    "\n",
    "        if self.verbose:\n",
    "            print(\"Model configuration:\")\n",
    "            print(f\"\\tModel_name: {self.model_name}\")\n",
    "            print(f\"\\tOptimizer: {self.optim}\")\n",
    "            print(f\"\\tLR scheduler: {lr_schedule}\")\n",
    "            print(f\"\\tLoss: {loss_funct}\")\n",
    "            print(f\"\\tEval: {eval_metric}\")\n",
    "            print(\n",
    "                \"Trainable parameteres: {} \\nNon trainable parameters: {} \\nTotal parameters: {}\".format(\n",
    "                    trainableParams, nonTrainableParams, totalParams\n",
    "                )\n",
    "            )\n",
    "\n",
    "        if self.model_name == \"cddpm\":\n",
    "            # calculate mean and variance of training dataset for normalization\n",
    "            model.normalizer.adapt(self.train_generator.map(lambda x, y: x))\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        print(\"Training is going to start:\")\n",
    "        history = model.fit(\n",
    "            self.train_generator,\n",
    "            validation_data=self.val_generator,\n",
    "            epochs=self.num_epochs,\n",
    "            callbacks=callbacks,\n",
    "        )\n",
    "\n",
    "        dt = time.time() - start\n",
    "        mins, sec = divmod(dt, 60)\n",
    "        hour, mins = divmod(mins, 60)\n",
    "        print(\n",
    "            \"\\nTime elapsed:\", hour, \"hour(s)\", mins, \"min(s)\", round(sec), \"sec(s)\\n\"\n",
    "        )\n",
    "\n",
    "        model.save_weights(os.path.join(self.saving_path, \"weights_last.h5\"))\n",
    "        self.history = history\n",
    "\n",
    "        os.makedirs(self.saving_path + \"/train_metrics\", exist_ok=True)\n",
    "\n",
    "        for key in history.history:\n",
    "            np.save(\n",
    "                self.saving_path + \"/train_metrics/\" + key + \".npy\",\n",
    "                history.history[key],\n",
    "            )\n",
    "        np.save(self.saving_path + \"/train_metrics/time.npy\", np.array([dt]))\n",
    "\n",
    "    def predict_images(self):\n",
    "        ground_truths = []\n",
    "        widefields = []\n",
    "        predictions = []\n",
    "        print(\"Prediction is going to start:\")\n",
    "        for test_filename in self.test_filenames:\n",
    "            lr_images, hr_images, _ = datasets.extract_random_patches_from_folder(\n",
    "                hr_data_path=self.test_hr_path,\n",
    "                lr_data_path=self.test_lr_path,\n",
    "                filenames=[test_filename],\n",
    "                scale_factor=self.scale_factor,\n",
    "                crappifier_name=self.crappifier_method,\n",
    "                lr_patch_shape=None,\n",
    "                datagen_sampling_pdf=1,\n",
    "            )\n",
    "\n",
    "            hr_images = np.expand_dims(hr_images, axis=-1)\n",
    "            lr_images = np.expand_dims(lr_images, axis=-1)\n",
    "\n",
    "            ground_truths.append(hr_images[0, ...])\n",
    "            widefields.append(lr_images[0, ...])\n",
    "            \n",
    "            if self.model_name == \"unet\":\n",
    "                if self.verbose:\n",
    "                    print(\"Padding will be added to the images.\")\n",
    "                    print(\"LR images before padding:\")\n",
    "                    print(\n",
    "                        \"LR images - shape:{} dtype:{}\".format(\n",
    "                            lr_images.shape, lr_images.dtype\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                height_padding, width_padding = utils.calculate_pad_for_Unet(\n",
    "                    lr_img_shape=lr_images[0].shape,\n",
    "                    depth_Unet=self.config.used_model.depth,\n",
    "                    is_pre=True,\n",
    "                    scale=self.scale_factor,\n",
    "                )\n",
    "\n",
    "                if self.verbose and (\n",
    "                    height_padding == (0, 0) and width_padding == (0, 0)\n",
    "                ):\n",
    "                    print(\"No padding has been needed to be added.\")\n",
    "\n",
    "                lr_images = utils.add_padding_for_Unet(\n",
    "                    lr_imgs=lr_images,\n",
    "                    height_padding=height_padding,\n",
    "                    width_padding=width_padding,\n",
    "                )\n",
    "\n",
    "            if self.verbose:\n",
    "                print(\n",
    "                    \"HR images - shape:{} dtype:{}\".format(\n",
    "                        hr_images.shape, hr_images.dtype\n",
    "                    )\n",
    "                )\n",
    "                print(\n",
    "                    \"LR images - shape:{} dtype:{}\".format(\n",
    "                        lr_images.shape, lr_images.dtype\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            if self.config.model.others.positional_encoding:\n",
    "                lr_images = utils.concatenate_encoding(\n",
    "                    lr_images,\n",
    "                    self.config.model.others.positional_encoding_channels,\n",
    "                )\n",
    "\n",
    "            optim = optimizer_scheduler_utils.select_optimizer(\n",
    "                library_name=self.library_name,\n",
    "                optimizer_name=self.optimizer_name,\n",
    "                learning_rate=self.learning_rate,\n",
    "                check_point=None,\n",
    "                parameters=None,\n",
    "                additional_configuration=self.config,\n",
    "            )\n",
    "\n",
    "            model = model_utils.select_model(\n",
    "                model_name=self.model_name,\n",
    "                input_shape=lr_images.shape,\n",
    "                output_channels=hr_images.shape[-1],\n",
    "                scale_factor=self.scale_factor,\n",
    "                datagen_sampling_pdf=self.datagen_sampling_pdf,\n",
    "                model_configuration=self.config.used_model,\n",
    "            )\n",
    "\n",
    "            loss_funct = \"mean_absolute_error\"\n",
    "            eval_metric = \"mean_squared_error\"\n",
    "\n",
    "            model.compile(\n",
    "                optimizer=optim, loss=loss_funct, metrics=[eval_metric, utils.ssim_loss]\n",
    "            )\n",
    "\n",
    "            # Load old weights\n",
    "            model.load_weights(os.path.join(self.saving_path, \"weights_best.h5\"))\n",
    "\n",
    "            aux_prediction = model.predict(lr_images, batch_size=1)\n",
    "\n",
    "            if self.model_name == \"unet\":\n",
    "                aux_prediction = utils.remove_padding_for_Unet(\n",
    "                    pad_hr_imgs=aux_prediction,\n",
    "                    height_padding=height_padding,\n",
    "                    width_padding=width_padding,\n",
    "                    scale=self.scale_factor,\n",
    "                )\n",
    "\n",
    "            aux_prediction = datasets.normalization(aux_prediction)\n",
    "\n",
    "            predictions.append(aux_prediction[0, ...])\n",
    "\n",
    "        self.Y_test = ground_truths\n",
    "        self.predictions = predictions\n",
    "        self.X_test = widefields\n",
    "\n",
    "        assert np.max(self.Y_test) <= 1.0 and np.max(self.predictions) <= 1.0 and np.max(self.X_test) <= 1.0\n",
    "        assert np.min(self.Y_test) >= 0.0 and np.min(self.predictions) >= 0.0 and np.min(self.X_test) >= 0.0 \n",
    "\n",
    "        if self.verbose:\n",
    "            utils.print_info(\"predict_images() - Y_test\", self.Y_test)\n",
    "            utils.print_info(\"predict_images() - predictions\", self.predictions)\n",
    "            utils.print_info(\"predict_images() - X_test\", self.X_test)\n",
    "\n",
    "        # Save the predictions\n",
    "        os.makedirs(self.saving_path + \"/predicted_images\", exist_ok=True)\n",
    "\n",
    "        for i, image in enumerate(predictions):\n",
    "            tf.keras.preprocessing.image.save_img(\n",
    "                self.saving_path + \"/predicted_images/\" + self.test_filenames[i],\n",
    "                image,\n",
    "                data_format=None,\n",
    "                file_format=None,\n",
    "            )\n",
    "        print(\n",
    "            \"Predicted images have been saved in: \"\n",
    "            + self.saving_path\n",
    "            + \"/predicted_images\"\n",
    "        )\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "microscopy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
